{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJosXpYXKMgE"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization, SpatialDropout2D\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import average_precision_score, PrecisionRecallDisplay\n",
        "from sklearn.utils import class_weight\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upload kaggle.json and set permissions\n",
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Step 2: Download dataset\n",
        "!kaggle datasets download -d gray8ed/audio-dataset-of-low-flying-aircraft-aerosonicdb\n",
        "\n",
        "# Step 3: Unzip dataset\n",
        "!unzip audio-dataset-of-low-flying-aircraft-aerosonicdb.zip -d ./audio_data\n",
        "\n",
        "# Step 4: Use it\n",
        "import os\n",
        "DATA_DIR = './audio_data'\n",
        "print(os.listdir(DATA_DIR))"
      ],
      "metadata": {
        "id": "Xis2c2dxNQMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/audio_data'\n",
        "\n",
        "# take a look at the directory files and structure\n",
        "print(os.listdir(DATA_DIR))\n",
        "print(os.listdir(DATA_DIR + '/audio'))\n",
        "print(os.listdir(DATA_DIR + '/audio/audio'))\n",
        "print(os.listdir(DATA_DIR + '/env_audio'))\n",
        "print(os.listdir(DATA_DIR + '/env_audio/env_audio'))"
      ],
      "metadata": {
        "id": "CGIUQOe4No0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set a path to the audio/audio directory\n",
        "AUDIO_DIR = os.path.join(DATA_DIR, 'audio/audio')\n",
        "\n",
        "# set a path to the env_audio/env_audio directory\n",
        "ENV_DIR = os.path.join(DATA_DIR, 'env_audio/env_audio')"
      ],
      "metadata": {
        "id": "fuMGTfYLNviK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the audio directory,\n",
        "# how many negative class \"0\", how many positive \"1\"?\n",
        "print(os.listdir(AUDIO_DIR))\n",
        "\n",
        "for i in ['0', '1']:\n",
        "    dir_files = len(os.listdir(os.path.join(AUDIO_DIR, i)))\n",
        "    print(f'Class {i} contains {dir_files} samples')"
      ],
      "metadata": {
        "id": "8nn5qpXINvqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the sample_meta.csv file for a look\n",
        "df = pd.read_csv(os.path.join(DATA_DIR, 'sample_meta.csv'))\n",
        "# sanity check on the number of samples in each class\n",
        "df['class'].value_counts()"
      ],
      "metadata": {
        "id": "TKrVNatLN0-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at all of the columns/labels available for each sample\n",
        "df.columns"
      ],
      "metadata": {
        "id": "v5iIkGJxN1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Fetch a random file from each class\n",
        "random.seed(42)\n",
        "NEG_FILE = random.sample(os.listdir(os.path.join(AUDIO_DIR, '0')), 1)[0]\n",
        "POS_FILE = random.sample(os.listdir(os.path.join(AUDIO_DIR, '1')), 1)[0]\n",
        "print(NEG_FILE)\n",
        "print(POS_FILE)"
      ],
      "metadata": {
        "id": "Vg1qwvqyN1IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to build a filepath from a filename and class combination\n",
        "def get_audio_path(df, filename):\n",
        "    # locate the filename and fetch the corresponding class (\"fclass\" == file class)\n",
        "    fclass = df.loc[df['filename'] == filename, 'class'].values[0]\n",
        "    filepath = os.path.join(AUDIO_DIR, str(fclass), filename)\n",
        "    return filepath, fclass"
      ],
      "metadata": {
        "id": "r5xieABMOAgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the function above works with our example files\n",
        "print(get_audio_path(df=df, filename=POS_FILE))\n",
        "print(get_audio_path(df=df, filename=NEG_FILE))"
      ],
      "metadata": {
        "id": "D2KpPkjfOAqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load a file to play and show it's waveform\n",
        "def load_show_audio(filename):\n",
        "    path, fclass = get_audio_path(df=df, filename=filename)\n",
        "    signal, sr = librosa.load(path)\n",
        "    print(f'{filename} sample rate: {str(sr)}')\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    librosa.display.waveshow(y=signal, sr=sr)\n",
        "    plt.show()\n",
        "    return ipd.Audio(path)"
      ],
      "metadata": {
        "id": "zec1bw4IOAtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and play the positive/aircraft example\n",
        "load_show_audio(filename=POS_FILE)"
      ],
      "metadata": {
        "id": "0PWLr_nbOAw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and play the negative/silence example\n",
        "load_show_audio(filename=NEG_FILE)"
      ],
      "metadata": {
        "id": "Yudy6ZcNOSCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set some constants for feature extraction, training and inference\n",
        "SR = 22050 # sample rate of the audio files\n",
        "DURATION = 5 # length of a segment in seconds\n",
        "SAMPLES_PER_SEGMENT = SR*DURATION # the number of samples per segment we expect\n",
        "N_FFT = 2048 # approx frequency resolution of 21.5 Hz\n",
        "HOP_LENGTH = 1024\n",
        "EXP_VECTORS_PER_SEGMENT = math.floor(SAMPLES_PER_SEGMENT/HOP_LENGTH)\n",
        "N_MELS = 128 # the number of frequency bins for spectrogram\n",
        "EXP_INPUT_SHAPE = (N_MELS, EXP_VECTORS_PER_SEGMENT) # the expected shape of the spectrogram\n",
        "print('Expected spectrogram shape:', EXP_INPUT_SHAPE)"
      ],
      "metadata": {
        "id": "MxK4gNi_OSJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load a file and chop it into spectrograms equal to the segment length\n",
        "def audio_to_spectrogram(filename):\n",
        "    path, fclass = get_audio_path(df=df, filename=filename)\n",
        "    signal, sr = librosa.load(path)\n",
        "\n",
        "\n",
        "    if sr != SR:\n",
        "        raise ValueError('Sample rate mismatch between audio and target')\n",
        "\n",
        "    clip_segments = math.ceil(len(signal) / SAMPLES_PER_SEGMENT)\n",
        "\n",
        "    # empty list to hold the spectrograms for this clip\n",
        "    specs = []\n",
        "\n",
        "    for segment in range(clip_segments):\n",
        "\n",
        "        start = SAMPLES_PER_SEGMENT * segment\n",
        "        end = start + SAMPLES_PER_SEGMENT - HOP_LENGTH\n",
        "\n",
        "        spec = librosa.feature.melspectrogram(y=signal[start:end],\n",
        "                                              sr=sr, n_fft=N_FFT,\n",
        "                                              n_mels=N_MELS,\n",
        "                                              hop_length=HOP_LENGTH,\n",
        "                                              window='hann')\n",
        "\n",
        "        db_spec = librosa.power_to_db(spec, ref=0.0)\n",
        "\n",
        "        if db_spec.shape[1] == EXP_VECTORS_PER_SEGMENT:\n",
        "            specs.append(db_spec)\n",
        "\n",
        "        # if the clip is shorter than the segment, add zero padding to the right\n",
        "        elif db_spec.shape[1] < EXP_VECTORS_PER_SEGMENT:\n",
        "            n_short = EXP_VECTORS_PER_SEGMENT - db_spec.shape[1]\n",
        "            db_spec = np.pad(db_spec, [(0, 0), (0, n_short)], 'constant')\n",
        "            specs.append(db_spec)\n",
        "\n",
        "    return (specs, fclass)"
      ],
      "metadata": {
        "id": "sp6vWpJNOSNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check the segmentation, spectrogram and padding are working correctly on a single file\n",
        "specs, fclass = audio_to_spectrogram(POS_FILE)\n",
        "\n",
        "fig, axes = plt.subplots(1,len(specs), sharey='row', figsize=(11, 3))\n",
        "\n",
        "count = 0\n",
        "\n",
        "for spec in specs:\n",
        "    axes[count] = librosa.display.specshow(spec, ax=axes[count])\n",
        "    count += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QFXXKsPyOSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to apply min-max scaling to squeeze spectrogram values between 0 and 1\n",
        "def normalise_array(array):\n",
        "    array = np.asarray(array)\n",
        "    min_val = array.min()\n",
        "    max_val = array.max()\n",
        "\n",
        "    norm_array = (array - min_val) / (max_val - min_val)\n",
        "\n",
        "    return norm_array"
      ],
      "metadata": {
        "id": "RmOFBM8KOSYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to take a list of files and extract their features\n",
        "# -> array of features (X) and array of corresponding labels (y)\n",
        "def preprocess(file_list):\n",
        "\n",
        "    data = {'feature': [], 'label': []}\n",
        "\n",
        "    for file in file_list:\n",
        "        specs, fclass = audio_to_spectrogram(filename=file)\n",
        "\n",
        "        for spec in specs:\n",
        "            norm_spec = normalise_array(spec)\n",
        "            data['feature'].append(norm_spec)\n",
        "            data['label'].append(fclass)\n",
        "\n",
        "    X = np.asarray(data['feature'])\n",
        "    y = np.asarray(data['label'])\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "D2iSb47ZOmRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into training, validation and testing portions\n",
        "train = df['filename'].loc[(df['fold'] == '1') | (df['fold'] == '2') | (df['fold'] == '3')| (df['fold'] == '4')].reset_index(drop=True) # takes folds 1, 2, 3 and 4 for training\n",
        "val = df['filename'].loc[df['fold'] == '5'].reset_index(drop=True) # takes fold 5 for validation\n",
        "test = df['filename'].loc[(df['fold'] == 'test')].reset_index(drop=True) # held-out test set\n",
        "\n",
        "print(f'The \"TRAIN\" set contains {train.shape[0]} samples.')\n",
        "print(f'The \"VALIDATION\" set contains {val.shape[0]} samples.')\n",
        "print(f'The \"TEST\" set contains {test.shape[0]} samples.')"
      ],
      "metadata": {
        "id": "H23kWMPcOmXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the train set\n",
        "X_train, y_train = preprocess(train)\n",
        "\n",
        "# preprocess the validation set\n",
        "X_val, y_val = preprocess(val)\n",
        "\n",
        "# preprocess the validation set\n",
        "X_test, y_test = preprocess(test)"
      ],
      "metadata": {
        "id": "DJ0pNcM6Ombz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape of the output equals the expected shape of the spectrogram\n",
        "X_train[0].shape == EXP_INPUT_SHAPE"
      ],
      "metadata": {
        "id": "1Aso8qzmO0tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set a random seed for reproducability\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "\n",
        "\n",
        "# define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(128, 107,1)))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(SpatialDropout2D(0.5))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "model.add(SpatialDropout2D(0.5))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "model.add(SpatialDropout2D(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='BinaryCrossentropy', metrics=[tf.keras.metrics.AUC(curve='PR', name='PR-AUC')])\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "W7CxfkjdO1dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "hist = model.fit(x=X_train,\n",
        "                 y=y_train,\n",
        "                 epochs=50,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                 class_weight={0: 3, 1:1},\n",
        "                 verbose=1,\n",
        "                 batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "DJI1fXDiO1hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"aircraft_detector_model.h5\")"
      ],
      "metadata": {
        "id": "ExZnvcggO1ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(hist.history['loss'], 'r')\n",
        "plt.plot(hist.history['val_loss'], 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N36o1bkDPBW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('PR-AUC')\n",
        "plt.plot(hist.history['PR-AUC'], 'r')\n",
        "plt.plot(hist.history['val_PR-AUC'], 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DI-bcsLyPBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('aircraft_detector_model.h5')"
      ],
      "metadata": {
        "id": "BuiKHmE0PBeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "def preprocess_audio(file_path):\n",
        "    SR = 22050\n",
        "    DURATION = 5\n",
        "    SAMPLES_PER_SEGMENT = SR * DURATION\n",
        "    N_FFT = 2048\n",
        "    HOP_LENGTH = 1024\n",
        "    N_MELS = 128\n",
        "    EXP_VECTORS_PER_SEGMENT = int(np.floor(SAMPLES_PER_SEGMENT / HOP_LENGTH))\n",
        "\n",
        "    y, sr = librosa.load(file_path, sr=SR)\n",
        "\n",
        "    if len(y) < SAMPLES_PER_SEGMENT:\n",
        "        y = np.pad(y, (0, SAMPLES_PER_SEGMENT - len(y)), mode='constant')\n",
        "    else:\n",
        "        y = y[:SAMPLES_PER_SEGMENT]\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    # Normalize between 0 and 1\n",
        "    norm_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
        "\n",
        "    # Add batch and channel dimension for model input: (1, height, width, 1)\n",
        "    input_tensor = norm_spec[np.newaxis, :, :, np.newaxis].astype(np.float32)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "# Load your model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('aircraft_detector_model.h5')\n",
        "\n",
        "def predict_audio_class(file_path):\n",
        "    processed_audio = preprocess_audio(file_path)\n",
        "    prediction = model.predict(processed_audio)[0][0]\n",
        "    predicted_class = 1 if prediction >= 0.5 else 0\n",
        "    confidence = prediction\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/audio_data/audio/audio/1/7C1CE4_2023-05-09_13-01-23_2_1.wav'\n",
        "pred_class, conf = predict_audio_class(file_path)\n",
        "print(f'Predicted class: {pred_class} (1=aircraft, 0=no aircraft)')\n",
        "print(f'Confidence: {conf:.4f}')"
      ],
      "metadata": {
        "id": "51dvnMxvO1p6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}